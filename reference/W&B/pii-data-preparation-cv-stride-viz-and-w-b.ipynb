{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Preparation for PII Data Detection\n\nThis notebook shares my current approach to CV, striding, visualization and dataset versioning with W&B. \n\nYou may want to run it interactively or add W&B API key to the secrets to run it offline.\n\nYou can check out [the video from my live training session](https://www.youtube.com/watch?v=w4ZDwiSXMK0).\n\nI also saved the outputs to the [Kaggle dataset](https://www.kaggle.com/datasets/thedrcat/pii-detection-cv-split) if you want to import it in a Kaggle training notebook. ","metadata":{}},{"cell_type":"code","source":"import json\nimport pandas as pd\n\ntrain = json.load(open(\"../input/pii-detection-removal-from-educational-data/train.json\"))\ndf = pd.DataFrame(train)\n\nlen(train)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:09:45.193724Z","iopub.execute_input":"2024-02-09T13:09:45.194551Z","iopub.status.idle":"2024-02-09T13:09:47.691909Z","shell.execute_reply.started":"2024-02-09T13:09:45.194489Z","shell.execute_reply":"2024-02-09T13:09:47.690602Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"6807"},"metadata":{}}]},{"cell_type":"markdown","source":"# CV Split\n\nLet's start by checking out the distribution of labels across all training essays. ","metadata":{}},{"cell_type":"code","source":"def encode_labels(df):\n    df[\"unique_labels\"] = df[\"labels\"].apply(lambda x: list(set(\n        [l.split('-')[1] for l in x if l != 'O']\n         )))\n    # add 1-hot encoding\n    from sklearn.preprocessing import MultiLabelBinarizer\n\n    mlb = MultiLabelBinarizer()\n    one_hot_encoded = mlb.fit_transform(df['unique_labels'])\n    one_hot_df = pd.DataFrame(one_hot_encoded, columns=mlb.classes_)\n    df = pd.concat([df, one_hot_df], axis=1)\n    \n    # add 'OTHER' column\n    df['OTHER'] = df['unique_labels'].apply(lambda x: 1 if len(x) == 0 else 0)\n    \n    return df, list(mlb.classes_) + ['OTHER']\n\ndf, label_classes = encode_labels(df)\n\nfor col in label_classes:\n    print(f'{col}: {df[col].sum()}')","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:10:15.838967Z","iopub.execute_input":"2024-02-09T13:10:15.839369Z","iopub.status.idle":"2024-02-09T13:10:16.916279Z","shell.execute_reply.started":"2024-02-09T13:10:15.839340Z","shell.execute_reply":"2024-02-09T13:10:16.914885Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"EMAIL: 24\nID_NUM: 33\nNAME_STUDENT: 891\nPHONE_NUM: 4\nSTREET_ADDRESS: 2\nURL_PERSONAL: 72\nUSERNAME: 5\nOTHER: 5862\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I want all the very rare classes to be in my validation split. This is going to be an opinionated split, but I'd like to pick the following numbers into my validation: ","metadata":{}},{"cell_type":"code","source":"# Shuffle the dataframe\ndf = df.sample(frac=1, random_state=42)\n\n# Create a 'valid' column and set it to False\ndf['valid'] = False\n\n# Define the validation numbers\nval_nums = {\n    'EMAIL': 12,\n    'ID_NUM': 12,\n    'NAME_STUDENT': 100,\n    'PHONE_NUM': 4,\n    'STREET_ADDRESS': 2,\n    'URL_PERSONAL': 20,\n    'USERNAME': 5,\n    'OTHER': 1000, \n}\n\n# For each class in val_nums, randomly select the specified number of examples and set 'valid' to True\nfor label, num in val_nums.items():\n    valid_indices = df[df[label] == 1].sample(n=num, random_state=42).index\n    df.loc[valid_indices, 'valid'] = True\n\n\n# Let's double check the classes per split:\nfor col in label_classes:\n    print(f'VALID {col}: {df[df.valid == True][col].sum()}')\n    print(f'TRAIN {col}: {df[df.valid == False][col].sum()}')","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:11:39.238322Z","iopub.execute_input":"2024-02-09T13:11:39.238872Z","iopub.status.idle":"2024-02-09T13:11:39.312482Z","shell.execute_reply.started":"2024-02-09T13:11:39.238836Z","shell.execute_reply":"2024-02-09T13:11:39.310745Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"VALID EMAIL: 13\nTRAIN EMAIL: 11\nVALID ID_NUM: 13\nTRAIN ID_NUM: 20\nVALID NAME_STUDENT: 124\nTRAIN NAME_STUDENT: 767\nVALID PHONE_NUM: 4\nTRAIN PHONE_NUM: 0\nVALID STREET_ADDRESS: 2\nTRAIN STREET_ADDRESS: 0\nVALID URL_PERSONAL: 26\nTRAIN URL_PERSONAL: 46\nVALID USERNAME: 5\nTRAIN USERNAME: 0\nVALID OTHER: 1000\nTRAIN OTHER: 4862\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Visualization\n\nLet's prepare the visualization code based on [this great notebook](https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy).","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/sinchir0/visualization-code-using-displacy\nimport spacy\nfrom spacy.tokens import Span\nfrom spacy import displacy\n\nnlp = spacy.blank(\"en\")\n\noptions = {\n    \"colors\": {\n        \"B-NAME_STUDENT\": \"aqua\",\n        \"I-NAME_STUDENT\": \"skyblue\",\n        \"B-EMAIL\": \"limegreen\",\n        \"I-EMAIL\": \"lime\",\n        \"B-USERNAME\": \"hotpink\",\n        \"I-USERNAME\": \"lightpink\",\n        \"B-ID_NUM\": \"purple\",\n        \"I-ID_NUM\": \"rebeccapurple\",\n        \"B-PHONE_NUM\": \"red\",\n        \"I-PHONE_NUM\": \"salmon\",\n        \"B-URL_PERSONAL\": \"silver\",\n        \"I-URL_PERSONAL\": \"lightgray\",\n        \"B-STREET_ADDRESS\": \"brown\",\n        \"I-STREET_ADDRESS\": \"chocolate\",\n    }\n}\n\ndef visualize(row):\n    doc = nlp(row.full_text)\n    doc.ents = [\n        Span(doc, idx, idx + 1, label=label)\n        for idx, label in enumerate(row.labels)\n        if label != \"O\"\n    ]\n    html = displacy.render(doc, style=\"ent\", jupyter=False, options=options)\n    return html\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:39:14.455127Z","iopub.execute_input":"2024-02-09T13:39:14.455535Z","iopub.status.idle":"2024-02-09T13:39:18.656520Z","shell.execute_reply.started":"2024-02-09T13:39:14.455507Z","shell.execute_reply":"2024-02-09T13:39:18.655297Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# from IPython.core.display import display, HTML\n# html = visualize(df.loc[0])\n# display(HTML(html))","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:13:37.475603Z","iopub.execute_input":"2024-02-09T13:13:37.476116Z","iopub.status.idle":"2024-02-09T13:13:37.481129Z","shell.execute_reply.started":"2024-02-09T13:13:37.476082Z","shell.execute_reply":"2024-02-09T13:13:37.479981Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Truncation with stride\n\nThere are two ways to do striding here - the best is probably to use tokenizers striding method. I opted for the easy way here and applied striding using spacy tokens. This means we're still facing variable sequence length after tokenization.","metadata":{}},{"cell_type":"code","source":"def add_token_indices(doc_tokens):\n    token_indices = list(range(len(doc_tokens)))\n    return token_indices\n\ndf['token_indices'] = df['tokens'].apply(add_token_indices)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:32:28.607245Z","iopub.execute_input":"2024-02-09T13:32:28.607642Z","iopub.status.idle":"2024-02-09T13:32:28.808179Z","shell.execute_reply.started":"2024-02-09T13:32:28.607614Z","shell.execute_reply":"2024-02-09T13:32:28.807216Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def rebuild_text(tokens, trailing_whitespace):\n    text = ''\n    for token, ws in zip(tokens, trailing_whitespace):\n        ws = \" \" if ws == True else \"\"\n        text += token + ws\n    return text\n\n\ndef split_rows(df, max_length, doc_stride):\n    new_df = []\n    for _, row in df.iterrows():\n        tokens = row['tokens']\n        if len(tokens) > max_length:\n            start = 0\n            while start < len(tokens):\n                remaining_tokens = len(tokens) - start\n                if remaining_tokens < max_length and start != 0:\n                    # Adjust start for the last window to ensure it has max_length tokens\n                    start = max(0, len(tokens) - max_length)\n                end = min(start + max_length, len(tokens))\n                new_row = {}\n                new_row['document'] = row['document']\n                new_row['valid'] = row['valid']\n                new_row['tokens'] = tokens[start:end]\n                new_row['trailing_whitespace'] = row['trailing_whitespace'][start:end]\n                new_row['labels'] = row['labels'][start:end]\n                new_row['token_indices'] = list(range(start, end))\n                new_row['full_text'] = rebuild_text(new_row['tokens'], new_row['trailing_whitespace'])\n                new_df.append(new_row)\n                if remaining_tokens >= max_length:\n                    start += doc_stride\n                else:\n                    # Break the loop if we've adjusted for the last window\n                    break\n        else:\n            new_row = {\n                'document': row['document'], \n                'valid': row['valid'],\n                'tokens': row['tokens'], \n                'trailing_whitespace': row['trailing_whitespace'], \n                'labels': row['labels'], \n                'token_indices': row['token_indices'], \n                'full_text': row['full_text']\n            }\n            new_df.append(new_row)\n    return pd.DataFrame(new_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:33:30.805559Z","iopub.execute_input":"2024-02-09T13:33:30.805994Z","iopub.status.idle":"2024-02-09T13:33:30.824196Z","shell.execute_reply.started":"2024-02-09T13:33:30.805964Z","shell.execute_reply":"2024-02-09T13:33:30.823204Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"max_length = 750\ndoc_stride = 250\nstride_df = split_rows(df, max_length, doc_stride)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:33:31.387110Z","iopub.execute_input":"2024-02-09T13:33:31.387494Z","iopub.status.idle":"2024-02-09T13:33:35.746754Z","shell.execute_reply.started":"2024-02-09T13:33:31.387466Z","shell.execute_reply":"2024-02-09T13:33:35.745318Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"len(df), len(stride_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:33:38.199329Z","iopub.execute_input":"2024-02-09T13:33:38.199735Z","iopub.status.idle":"2024-02-09T13:33:38.207354Z","shell.execute_reply.started":"2024-02-09T13:33:38.199706Z","shell.execute_reply":"2024-02-09T13:33:38.206098Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(6807, 11468)"},"metadata":{}}]},{"cell_type":"code","source":"stride_df, label_classes = encode_labels(stride_df)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:33:49.155654Z","iopub.execute_input":"2024-02-09T13:33:49.156079Z","iopub.status.idle":"2024-02-09T13:33:49.557650Z","shell.execute_reply.started":"2024-02-09T13:33:49.156050Z","shell.execute_reply":"2024-02-09T13:33:49.556532Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Saving to W&B\n\nIt's best practice to version datasets properly and visualize them in W&B. Let's do this!\n\nTo run below code, please add your `WANDB_API_KEY` secret to Kaggle notebook secrets. You can get it [here](https://wandb.ai/authorize).","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nimport wandb\n\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=wandb_api_key)\nwandb.init(project='pii', job_type='preprocessing')","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:35:51.843764Z","iopub.execute_input":"2024-02-09T13:35:51.844302Z","iopub.status.idle":"2024-02-09T13:36:28.132004Z","shell.execute_reply.started":"2024-02-09T13:35:51.844267Z","shell.execute_reply":"2024-02-09T13:36:28.130703Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarek\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240209_133556-ku2o5r8l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/darek/pii/runs/ku2o5r8l' target=\"_blank\">exalted-hill-61</a></strong> to <a href='https://wandb.ai/darek/pii' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/darek/pii' target=\"_blank\">https://wandb.ai/darek/pii</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/darek/pii/runs/ku2o5r8l' target=\"_blank\">https://wandb.ai/darek/pii/runs/ku2o5r8l</a>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/darek/pii/runs/ku2o5r8l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7c140f133700>"},"metadata":{}}]},{"cell_type":"code","source":"# Let's add our hyperparameters to the config \n\nwandb.config.update({\n    'max_length': max_length,\n    'doc_stride': doc_stride,\n})","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:36:28.134712Z","iopub.execute_input":"2024-02-09T13:36:28.136203Z","iopub.status.idle":"2024-02-09T13:36:28.622474Z","shell.execute_reply.started":"2024-02-09T13:36:28.136155Z","shell.execute_reply":"2024-02-09T13:36:28.621065Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Let's first log data as artifacts\n\ndf.to_parquet('raw_data.parquet', index=False)\nstride_df.to_parquet('stride_data.parquet', index=False)\n\nraw_data = wandb.Artifact(name=\"raw_data\", type=\"dataset\")\nraw_data.add_file('raw_data.parquet')\nwandb.log_artifact(raw_data)\n\nprocessed_data = wandb.Artifact(name=\"processed_data\", type=\"dataset\")\nprocessed_data.add_file('stride_data.parquet')\nwandb.log_artifact(processed_data)","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:36:54.464200Z","iopub.execute_input":"2024-02-09T13:36:54.464701Z","iopub.status.idle":"2024-02-09T13:37:06.015645Z","shell.execute_reply.started":"2024-02-09T13:36:54.464645Z","shell.execute_reply":"2024-02-09T13:37:06.014373Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<Artifact processed_data>"},"metadata":{}}]},{"cell_type":"code","source":"# We will generate html viz for every train essay, wrap it up in `wandb.Html` and create a W&B table to inspect it\nwandb_htmls = [wandb.Html(visualize(row)) for _, row in df.iterrows()]\ndf['visualization'] = wandb_htmls\ntable = wandb.Table(dataframe=df)\nwandb.log({'original_dataset': table})","metadata":{"execution":{"iopub.status.busy":"2024-02-09T13:39:23.147056Z","iopub.execute_input":"2024-02-09T13:39:23.147548Z","iopub.status.idle":"2024-02-09T13:46:56.235958Z","shell.execute_reply.started":"2024-02-09T13:39:23.147512Z","shell.execute_reply":"2024-02-09T13:46:56.234661Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/spacy/displacy/__init__.py:213: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n  warnings.warn(Warnings.W006)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Finish W&B run\nwandb.finish()","metadata":{},"execution_count":20,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"771c804215bf443cb4bd9108a7da7f3e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='30.090 MB of 30.090 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":[" View run <strong style=\"color:#cdcd00\">rosy-monkey-34</strong> at: <a href='https://wandb.ai/darek/pii/runs/46cown4z' target=\"_blank\">https://wandb.ai/darek/pii/runs/46cown4z</a><br/> View job at <a href='https://wandb.ai/darek/pii/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNzYwNDI4Nw==/version_details/v5' target=\"_blank\">https://wandb.ai/darek/pii/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjEzNzYwNDI4Nw==/version_details/v5</a><br/>Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["Find logs at: <code>./wandb/run-20240207_235806-46cown4z/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}}]},{"cell_type":"markdown","source":"# Share your findings\n\nIf you find some good insights from inspecting the data, please share in the comments!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}